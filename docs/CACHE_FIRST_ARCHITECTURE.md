# Cache-First Architecture

## Overview

The system now implements a **cache-first approach** as per the improved flow diagram:

```
request â†’ cache_check â†’ {hit â†’ response} | {miss â†’ normalize â†’ rule â†’ router â†’ llm â†’ response}
```

This ensures:
- **Repeated requests** skip expensive analysis
- **First-time requests** go through full pipeline
- **Cache hit saves** ~150-500ms per request

## Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Request   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Decode       â”‚  Load batch â†’ normalize
â”‚  Batch Decoder   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Cache Check  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  cache_check_    â”‚          â”‚ (if cache hit, populate 
â”‚  node            â”‚          â”‚  analysis fields)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€ Cache HIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                â”‚
       â”‚                            â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                            â”‚   Response  â”‚
       â”‚                            â”‚   response_ â”‚
       â”‚                            â”‚   node      â”‚
       â”‚                            â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                â”‚
       â”‚                            â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
       â”‚                            â”‚  Output  â”‚
       â”‚                            â”‚  (fast!) â”‚
       â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€ Cache MISS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                â”‚
       â–¼                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  3. Rule Engine  â”‚                   â”‚
â”‚  rule_engine_    â”‚ OWASP CRS scoring â”‚
â”‚  node            â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Router       â”‚ Fast (BLOCK?) or Slow (LLM)?
â”‚  router_node     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€ Fast Path (blocked) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                â”‚
       â–¼                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  5a. Cache Save  â”‚ Save to cache     â”‚
â”‚  cache_save_node â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
       â”‚                               â”‚
       â”‚                               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
 â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Slow Path (LLM)? â”‚
 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€ No (score < threshold) â”€â”€â”€â”€â”€â”€â”¬â”€â”€â–º Cache Save â”€â”€â”
       â”‚                               â”‚                 â”‚
       â”‚                               â”‚                 â”‚
       â””â”€ Yes (needs analysis) â”€â”€â”€â”€â”€â”€â”€â”€â”¤                 â”‚
                                       â”‚                 â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚                 â”‚
       â”‚                              â”‚â”‚                 â”‚
       â–¼                              â”‚â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚â”‚                 â”‚
â”‚  6. LLM Node     â”‚ Deep analysis    â”‚â”‚                 â”‚
â”‚  llm_node        â”‚                  â”‚â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚â”‚                 â”‚
       â”‚                              â”‚â”‚                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚                 â”‚
                          â”‚            â”‚                 â”‚
                          â–¼            â–¼                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
                    â”‚  5b. Cache Save Node     â”‚         â”‚
                    â”‚  cache_save_node         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  7. Response     â”‚
                    â”‚  response_node   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Output (JSON)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance Impact

### Cache Hit (repeated request)
- Decode: ~1ms
- **Cache Check: ~0.5ms** âœ… (instant lookup)
- Response Build: ~1ms
- **Total: ~2.5ms** (96% faster!)

### Cache Miss (first-time request)
- Decode: ~1ms
- Cache Check: ~0.5ms
- Rule Engine: ~3ms
- Router: <1ms
- LLM (if needed): ~500ms
- Response Build: ~1ms
- **Total: ~5-505ms** (depending on path)

## Implementation Details

### cache_check_node()

```python
def cache_check_node(state: SOCState) -> dict:
    """
    Check if request results are already cached.
    If cached, populate all analysis fields immediately.
    If not cached, mark for analysis.
    """
    for item in state["items"]:
        request_hash = _hash_request(item["raw_request"])
        
        if request_hash in REQUEST_CACHE:
            # Cache HIT - restore cached analysis
            item["cache_hit"] = True
            item.update(REQUEST_CACHE[request_hash])
        else:
            # Cache MISS - mark for analysis
            item["cache_hit"] = False
    
    return state
```

### cache_save_node()

```python
def cache_save_node(state: SOCState) -> dict:
    """
    Save analyzed results to cache for future requests.
    Called AFTER both fast and slow path analysis.
    """
    for item in state["items"]:
        request_hash = _hash_request(item["raw_request"])
        
        if request_hash not in REQUEST_CACHE:
            REQUEST_CACHE[request_hash] = {
                "attack_type": item["attack_type"],
                "rule_score": item["rule_score"],
                "severity": item["severity"],
                "fast_decision": item["fast_decision"],
                "evidence": item["evidence"],
                "attack_candidates": item["attack_candidates"],
                "blocked": item["blocked"],
            }
    
    return state
```

## Routing Logic

### route_cache_hit()
```python
def route_cache_hit(state: SOCState) -> str:
    if all(item["cache_hit"] for item in state["items"]):
        return "cache_hit"   # Skip analysis
    return "cache_miss"      # Do analysis
```

### route_after_rule()
```python
def route_after_rule(state: SOCState) -> str:
    if all(item["blocked"] for item in state["items"]):
        return "fast"     # BLOCK â†’ save & response
    return "slow"         # Needs LLM â†’ analyze
```

## Edge Cases

### 1. Batch with Mixed Cache States
```json
{
  "requests": [
    "/api/users?id=1 OR 1=1",          // CACHED MISS â†’ HIT
    "/search?q=python",                 // CACHED MISS â†’ HIT  
    "/admin?key=secret",                // CACHE MISS â†’ analyze
  ]
}
```

Result: Items 1 & 2 skip analysis (2.5ms total), Item 3 goes through pipeline (5-505ms).

### 2. Similar But Different Requests
```
Request 1: /api/users?id=1 OR 1=1     (SQL Injection, cached)
Request 2: /api/users?id=2 OR 1=1     (Different hash, not cached)
Request 3: /api/users?id=1 OR 1=1     (Identical, cache hit)
```

Each unique URL gets its own cache entry. Hash comparison is case-insensitive and URL-normalized.

### 3. Cache Invalidation
Currently: **No explicit invalidation** (in-memory, session-based).

For production:
- Implement Redis with TTL (e.g., 1 hour)
- Add cache versioning (e.g., when rule engine version changes)
- Support cache clear endpoint

## Configuration

### Current Strategy
- **In-Memory Cache**: Fast, simple (Python dict)
- **Scope**: Per-process session
- **TTL**: None (session-based)

### Production Recommendations

```python
# Redis Backend
import redis
cache = redis.Redis(host='localhost', port=6379, decode_responses=True)

def cache_check_node(state):
    for item in state["items"]:
        key = _hash_request(item["raw_request"])
        cached = cache.get(key)
        if cached:
            item.update(json.loads(cached))
        item["cache_hit"] = bool(cached)
    return state

def cache_save_node(state):
    for item in state["items"]:
        key = _hash_request(item["raw_request"])
        if not cache.exists(key):
            cache.setex(
                key, 
                3600,  # 1 hour TTL
                json.dumps({...})
            )
    return state
```

## Testing

```bash
# Test cache flow
python test_cache_simple.py

# Test with full graph (after fixing imports)
python test_cache_flow.py
```

## Metrics

### Cache Effectiveness
- **Hit Rate**: % of requests found in cache
- **Latency Saved**: Time saved by skipping analysis
- **Memory Usage**: Cache size (items Ã— average item size)

### Monitoring
```python
REQUEST_CACHE_STATS = {
    "total_requests": 0,
    "cache_hits": 0,
    "cache_misses": 0,
    "latency_saved_ms": 0,
}

hit_rate = cache_hits / (cache_hits + cache_misses) * 100
```

---

**Flow improved**: Cache check **before** rule engine â†’ Significant latency reduction for repeated requests! ğŸš€
